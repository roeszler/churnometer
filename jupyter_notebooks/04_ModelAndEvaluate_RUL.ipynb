{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CI Portfolio Project 5 - Filter Maintenance Predictor 2022\n",
    "## **ML Model - Predict Remaining Useful Life (RUL)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "Answer [Business Requirement 1](https://github.com/roeszler/filter-maintenance-predictor/blob/main/README.md#business-requirements) :\n",
    "*   Fit and evaluate a **regression model** to predict the Remaining Useful Life of a replaceable part\n",
    "*   Fit and evaluate a **classification model** to predict the Remaining Useful Life of a replaceable part should the regressor not perform well.\n",
    "\n",
    "## Inputs\n",
    "\n",
    "Data cleaning:\n",
    "* outputs/datasets/cleaned/dfCleanTotal.csv\n",
    "\n",
    "## Outputs\n",
    "\n",
    "* Train set (features and target)\n",
    "* Test set (features and target)\n",
    "* Validation set (features and target)\n",
    "* ML pipeline to predict RUL\n",
    "* A map of the labels\n",
    "* Feature Importance Plot\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"Current directory set to new location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The major steps in this Regressor Pipeline\n",
    "\n",
    "1. **ML Pipeline: Regressor**\n",
    "    * Create Regressor Pipeline\n",
    "    * Split the train set\n",
    "    * Grid Search CV SKLearn\n",
    "        * Use standard hyperparameters to find most suitable algorithm\n",
    "        * Extensive search on most suitable algorithm to find the best hyperparameter configuration\n",
    "    * Assess Feature Performance\n",
    "    * Evaluate Regressor\n",
    "    * Create Train, Test, Validation Sets\n",
    "\n",
    "2. **ML Pipeline: Regressor + Principal Component Analysis (PCA)**\n",
    "    * Prepare the Data for the Pipeline\n",
    "    * Create Regressor + PCA Pipeline\n",
    "    * Split the train and validation sets\n",
    "    * Grid Search CV SKLearn\n",
    "        * Use standard hyperparameters to find most suitable algorithm\n",
    "        * Do an extensive search on most suitable algorithm to find the best hyperparameter configuration\n",
    "    * Assess Feature Performance\n",
    "    * Evaluate Regressor\n",
    "    * Create Train, Test, Validation Sets\n",
    "\n",
    "_Optionally_\n",
    "\n",
    "3. **Convert Regression to Classification**\n",
    "    * Convert numerical target to bins, and check if it is balanced\n",
    "    * Rewrite Pipeline for ML Modelling\n",
    "    * Load Algorithms For Classification\n",
    "    * Split the Train Test sets:\n",
    "    * Grid Search CV SKLearn:\n",
    "        * Use standard hyper parameters to find most suitable model\n",
    "        * Grid Search CV\n",
    "        * Check Result\n",
    "    * Do an extensive search on the most suitable model to find the best hyperparameter configuration.\n",
    "        * Define Model Parameters\n",
    "        * Extensive Grid Search CV                             \n",
    "        * Check Results\n",
    "        * Check Best Model\n",
    "        * Parameters for best model\n",
    "        * Define the best clf_pipeline\n",
    "    * Assess Feature Importance\n",
    "    * Evaluate Classifier on Train and Test Sets\n",
    "        * Custom Function\n",
    "        * List that relates the classes and tenure interval\n",
    "\n",
    "4. **Decide which pipeline to use**\n",
    "\n",
    "5. **Refit with the best features**\n",
    "    * Rewrite Pipeline\n",
    "    * Split Train Test Set with only best features\n",
    "    * Subset best features\n",
    "    * Grid Search CV SKLearn\n",
    "    * Best Parameters\n",
    "        * Manually\n",
    "    * Grid Search CV\n",
    "    * Check Results\n",
    "    * Check Best Model\n",
    "    * Define the best pipeline\n",
    "\n",
    "6. **Assess Feature Importance**\n",
    "\n",
    "7. **Push Files to Repo**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Modelling:\n",
    "The hypothesis part of the process where you will find out whether you can answer the question.\n",
    "* Identify what techniques to use.\n",
    "* Split your data into train, validate and test sets.\n",
    "* Build and train the models with the train data set.\n",
    "* Validate Models and hyper-parameter : Trial different machine learning methods and models with the validation data set.\n",
    "* Poor Results - return to data preparation for feature engineering\n",
    "* Successful hypothesis - where the inputs from the data set are mapped to the output target / label appropriately to evaluate.\n",
    "\n",
    "5. Evaluation:\n",
    "Where you test whether the model can predict unseen data.\n",
    "* Test Dataset\n",
    "* Choose the model that meets the business success criteria best.\n",
    "* Review and document the work that you have done.\n",
    "* If your project meets the success metrics you defined with your customer?\n",
    "- Ready to deploy. -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Cleaned Data\n",
    "The pipeline should handle the cleaning and engineering by itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import os\n",
    "# os.environ['KAGGLE_CONFIG_DIR'] = os.getcwd()\n",
    "# ! chmod 600 kaggle.json\n",
    "\n",
    "# KaggleDatasetPath = 'prognosticshse/preventive-to-predicitve-maintenance'\n",
    "# DestinationFolder = 'inputs/datasets/raw'   \n",
    "# ! kaggle datasets download -d {KaggleDatasetPath} -p {DestinationFolder}\n",
    "\n",
    "# ! unzip {DestinationFolder}/*.zip -d {DestinationFolder} \\\n",
    "#   && rm {DestinationFolder}/*.zip \\\n",
    "#   && rm {DestinationFolder}/*.pdf \\\n",
    "#   && rm {DestinationFolder}/*.mat \\\n",
    "# #   && rm kaggle.json\n",
    "\n",
    "# df_test = pd.read_csv(f'inputs/datasets/raw/Test_Data_CSV.csv')\n",
    "# df_train = pd.read_csv(f'inputs/datasets/raw/Train_Data_CSV.csv')\n",
    "# # df_total = pd.read_csv(f'outputs/datasets/cleaned/dfCleanTotal.csv')\n",
    "# df_total = pd.concat([df_train, df_test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df_total = pd.read_csv(f'outputs/datasets/cleaned/dfCleanTotal.csv')\n",
    "# df_total = pd.read_csv(f'outputs/datasets/transformed/dfTransformedTotal.csv') # data with negative log_EWM values removed\n",
    "# df_total = (pd.read_csv(\"outputs/datasets/collection/PredictiveMaintenanceTotal.csv\").drop(labels=['customerID', 'TotalCharges', 'Churn'], axis=1))\n",
    "print(df_total.shape)\n",
    "# df_total"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include 4 Point Exponentially Weighted Mean and Remove Negative Values\n",
    "\n",
    "**Note:** A runtime warning indicating a divide by zero is expected in the calculation of the log_EWM. We are using this to identify which values to delete, so have temporarily suppressed the warning for this calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Log EWM to identify values to remove\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df_means = pd.DataFrame()\n",
    "\n",
    "list_data_nos = list(df_total['Data_No'].unique())\n",
    "for n in list_data_nos:\n",
    "    if (df_total.Data_No != df_total.Data_No.shift(1)).any().any():\n",
    "        df_bin = df_total[df_total['Data_No'] == n]\n",
    "\n",
    "        ewm_calc = df_bin['Differential_pressure'].ewm(span=4, adjust=False).mean()\n",
    "        df_bin.insert(loc=2, column='4point_EWM', value=ewm_calc)\n",
    "\n",
    "        log_ewm = np.log(ewm_calc)\n",
    "        df_bin.insert(loc=3, column='log_EWM', value=log_ewm)\n",
    "\n",
    "        df_means = pd.concat([df_means, df_bin], ignore_index = True)\n",
    "df_total = df_means\n",
    "\n",
    "warnings.resetwarnings()\n",
    "\n",
    "# Delete Negatives\n",
    "data = df_total.loc[:, df_total.columns == 'log_EWM']\n",
    "df_total = df_total[data.select_dtypes(include=[np.number]).ge(-0).all(1)].reset_index(drop=True)\n",
    "\n",
    "# Delete engineered values\n",
    "del df_total['log_EWM']\n",
    "# del df_total['4point_EWM']\n",
    "\n",
    "# df_total.loc[391:397]\n",
    "# print(df_total.shape)\n",
    "df_total"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline : Regressor\n",
    "## Create Regressor Pipeline\n",
    "### Set the Transformations\n",
    "* Smart correlation\n",
    "* feat_scaling\n",
    "* feat_selection\n",
    "* Modelling\n",
    "* Model as variable\n",
    "\n",
    "Note: Numerical Transformation not required as data supplied as integers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ML Pipeline for **Fitting Models** (regression)\n",
    "Modelling and Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def PipelineOptimization(model):\n",
    "#     pipeline_base = Pipeline([\n",
    "#         # (\"OrdinalCategoricalEncoder\", OrdinalEncoder(encoding_method='arbitrary',\n",
    "#         #                                              variables=['gender', 'Partner', 'Dependents', 'PhoneService',\n",
    "#         #                                                         'MultipleLines', 'InternetService', 'OnlineSecurity',\n",
    "#         #                                                         'OnlineBackup', 'DeviceProtection', 'TechSupport',\n",
    "#         #                                                         'StreamingTV', 'StreamingMovies', 'Contract',\n",
    "#         #                                                         'PaperlessBilling', 'PaymentMethod'])),\n",
    "#         (\"SmartCorrelatedSelection\", SmartCorrelatedSelection(variables=None, method=\"spearman\", threshold=0.6, selection_method=\"variance\")),\n",
    "#         (\"feat_scaling\", StandardScaler()),\n",
    "#         (\"feat_selection\",  SelectFromModel(model)),\n",
    "#         (\"model\", model),\n",
    "#     ])\n",
    "#     return pipeline_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Management\n",
    "from sklearn.pipeline import Pipeline\n",
    "from feature_engine.encoding import OrdinalEncoder\n",
    "from feature_engine.selection import SmartCorrelatedSelection\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# ML regression algorithms\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, AdaBoostRegressor, ExtraTreesRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "\n",
    "# # ML classification algorithms\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, AdaBoostClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "def PipelineOptimization(model):\n",
    "    pipeline_base = Pipeline([\n",
    "        (\"OrdinalCategoricalEncoder\", OrdinalEncoder(encoding_method='arbitrary',\n",
    "                                                     variables=['Data_No', 'Differential_pressure', '4point_EWM', 'Flow_rate',\n",
    "                                                                'Time', 'Dust_feed', 'Dust', 'RUL', 'mass_g',\n",
    "                                                                'cumulative_mass_g', 'Tt', 'filter_balance'])),\n",
    "        ('SmartCorrelatedSelection', SmartCorrelatedSelection(variables=None, method=\"spearman\", threshold=0.6, selection_method=\"variance\")),\n",
    "        ('feat_scaling', StandardScaler()),\n",
    "        ('feat_selection', SelectFromModel(model)),\n",
    "        ('model', model),])\n",
    "    return pipeline_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = PipelineOptimization(self.models[key])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Custom Class** to fit a set of algorithms, each with its own set of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "class HyperparameterOptimizationSearch:\n",
    "\n",
    "    def __init__(self, models, params):\n",
    "        self.models = models\n",
    "        self.params = params\n",
    "        self.keys = models.keys()\n",
    "        self.grid_searches = {}\n",
    "\n",
    "    def fit(self, X, y, cv, n_jobs, verbose=1, scoring=None, refit=False):\n",
    "        for key in self.keys:\n",
    "            model = PipelineOptimization(self.models[key])\n",
    "            print(f\"\\nRunning GridSearchCV for {key} \\n\")\n",
    "\n",
    "            params = self.params[key]\n",
    "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs,\n",
    "                              verbose=verbose, scoring=scoring)\n",
    "            gs.fit(X, y)\n",
    "            self.grid_searches[key] = gs\n",
    "\n",
    "    def score_summary(self, sort_by='mean_score'):\n",
    "        def row(key, scores, params):\n",
    "            d = {\n",
    "                'estimator': key,\n",
    "                'min_score': min(scores),\n",
    "                'max_score': max(scores),\n",
    "                'mean_score': np.mean(scores),\n",
    "                'std_score': np.std(scores),\n",
    "            }\n",
    "            return pd.Series({**params, **d})\n",
    "\n",
    "        rows = []\n",
    "        for k in self.grid_searches:\n",
    "            params = self.grid_searches[k].cv_results_['params']\n",
    "            scores = []\n",
    "            for i in range(self.grid_searches[k].cv):\n",
    "                key = \"split{}_test_score\".format(i)\n",
    "                r = self.grid_searches[k].cv_results_[key]\n",
    "                scores.append(r.reshape(len(params), 1))\n",
    "\n",
    "            all_scores = np.hstack(scores)\n",
    "            for p, s in zip(params, all_scores):\n",
    "                rows.append((row(k, s, p)))\n",
    "\n",
    "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
    "\n",
    "        columns = ['estimator', 'min_score',\n",
    "                   'mean_score', 'max_score', 'std_score']\n",
    "        columns = columns + [c for c in df.columns if c not in columns]\n",
    "\n",
    "        return df[columns], self.grid_searches"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert `Dust` to floating variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dust_density_total = [0.900 if n == 'ISO 12103-1, A2 Fine Test Dust' else (1.025 if n == 'ISO 12103-1, A3 Medium Test Dust' else 1.200) for n in df_total['Dust']]\n",
    "df_total['Dust'] = dust_density_total"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add Filter Balance to **df_total**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total_dp = df_total['Differential_pressure']\n",
    "df_test_filter_balance = (((600 - df_total_dp)/600)*100).round(decimals = 2)\n",
    "df_total.insert(loc=8, column='filter_balance', value=df_test_filter_balance)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into Train, Test, Validate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is discrete however in bins, so:\n",
    "#### Extract Cleaned **Train** & **Test** Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = df_total['Data_No'].iloc[0:len(df_total)]\n",
    "df_train = df_total[n < 51].reset_index(drop=True)\n",
    "df_test = df_total[n > 50].reset_index(drop=True)\n",
    "del df_train['RUL']\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Engineer Even Distribution of `Dust` in **Train** dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_copy = df_train.copy()\n",
    "\n",
    "bin_sum = df_train_copy.groupby('Data_No')['Data_No'].count().reset_index(name='bin_tot')\n",
    "map_bin = df_train_copy['Data_No'].map(bin_sum.set_index('Data_No')['bin_tot'])\n",
    "df_train_copy.loc[:, 'bin_size'] = map_bin\n",
    "\n",
    "dust_A2 = df_train_copy[df_train_copy['Dust'] == 0.900]\n",
    "filter_A2 = dust_A2[dust_A2.Data_No != dust_A2.Data_No.shift(-1)]\n",
    "df_train_A2 = filter_A2.sort_values(by='filter_balance', ascending=True)\n",
    "df_train_A2['c_sum'] = df_train_A2['bin_size'].cumsum()\n",
    "# df_train_A2.head(13).style.hide(['Time', 'Dust_feed', 'Flow_rate', 'Dust', 'mass_g', 'cumulative_mass_g', 'Tt'], axis=\"columns\")\n",
    "\n",
    "dust_A3 = df_train_copy[df_train_copy['Dust'] == 1.025]\n",
    "filter_A3 = dust_A3[dust_A3.Data_No != dust_A3.Data_No.shift(-1)]\n",
    "df_train_A3 = filter_A3.sort_values(by='filter_balance', ascending=True)\n",
    "df_train_A3['c_sum'] = df_train_A3['bin_size'].cumsum()\n",
    "# dn_fb = df_train_A3.loc[:, 'Data_No'].head(14).sort_values(ascending=True).reset_index(drop=True)\n",
    "# df_train_A3.head(14).style.hide(['Time', 'Dust_feed', 'Flow_rate', 'Dust', 'mass_g', 'cumulative_mass_g', 'Tt'], axis=\"columns\")\n",
    "\n",
    "A2_bin = df_train_A2['Data_No'].head(9)\n",
    "A3_bin = df_train_A3['Data_No'].head(9)\n",
    "\n",
    "df_train_cleaned_A1 = df_train_copy[df_train_copy['Dust'] == 1.200]\n",
    "df_train_cleaned_A2 = df_train_copy[df_train_copy['Data_No'].isin(A2_bin)]\n",
    "df_train_cleaned_A3 = df_train_copy[df_train_copy['Data_No'].isin(A3_bin)]\n",
    "\n",
    "df_train_concat = pd.concat([df_train_cleaned_A1, df_train_cleaned_A2, df_train_cleaned_A3], ignore_index = True)\n",
    "df_train_model = df_train_concat.sort_values(by='Data_No', ascending=True)\n",
    "\n",
    "print('A1 Dust_train :', df_train_cleaned_A1.shape)\n",
    "print('A2 Dust_train :', df_train_cleaned_A2.shape)\n",
    "print('A3 Dust_train :', df_train_cleaned_A3.shape)\n",
    "print('\\nTotal df_train :', df_train_model.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine **Target** and **Independent** Variables and Extract **Validation** Dataset\n",
    "\n",
    "As discussed in the readme, this data has been supplied pre-split into **train** and **test** within unique **data bins**. \n",
    "We extract random observations from the **test** dataset to create a **validation** set, in a 70:30 split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from feature_engine.selection import SmartCorrelatedSelection\n",
    "# corr_sel = SmartCorrelatedSelection(variables=None, method=\"spearman\", threshold=0.6, selection_method=\"variance\")\n",
    "\n",
    "# df_engineering = df_test.copy()\n",
    "# corr_sel.fit_transform(df_engineering)\n",
    "# print('Correlated Variables :\\n', corr_sel.correlated_feature_sets_)\n",
    "# print('\\nFeatures to Drop :\\n', corr_sel.features_to_drop_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review correlations, Drop Features and Split into **70% test** and **30% validate**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from feature_engine.selection import SmartCorrelatedSelection\n",
    "\n",
    "corr_sel = SmartCorrelatedSelection(variables=None, method=\"spearman\", threshold=0.6, selection_method=\"variance\")\n",
    "df_engineering = df_test.copy()\n",
    "corr_sel.fit_transform(df_engineering)\n",
    "\n",
    "X = df_test.drop(corr_sel.features_to_drop_,axis=1)\n",
    "y = df_test['Differential_pressure']\n",
    "\n",
    "X_test, X_validate, y_test, y_validate = train_test_split(X,y,test_size=0.30, random_state=0)\n",
    "\n",
    "print(X_test.shape, 'X_test')\n",
    "print(X_validate.shape, 'X_validate')\n",
    "print(y_test.shape, 'y_test')\n",
    "print(y_validate.shape, 'y_validate')\n",
    "print('\\nFeatures Dropped :\\n', corr_sel.features_to_drop_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define **X_train**, **y_train** variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_sel = SmartCorrelatedSelection(variables=None, method=\"spearman\", threshold=0.6, selection_method=\"variance\")\n",
    "df_engineering = df_train.copy()\n",
    "corr_sel.fit_transform(df_engineering)\n",
    "\n",
    "X_train = df_train.drop(corr_sel.features_to_drop_,axis=1)\n",
    "y_train = df_train['Differential_pressure']\n",
    "\n",
    "print(X_train.shape, 'X_train')\n",
    "print(y_train.shape, 'y_train')\n",
    "print('\\nFeatures Dropped :\\n', corr_sel.features_to_drop_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Target Imbalance\n",
    "### No need to handle target imbalance in this **regression model**.\n",
    "* Typically we only need to create a single pipeline for Classification or Regression task. \n",
    "* The exception occurs when we need to handle a **classification target imbalance**, which requires more than one model to process "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the pipeline with Data\n",
    "* Prepare the data for handling the train set and\n",
    "* ~~Fix the target imbalance~~\n",
    "* Feature scaling\n",
    "* Feature selection\n",
    "* Modelling\n",
    "* Custom Python class for **hyperparameter optimization**\n",
    "\n",
    "### Use standard hyperparameters to find most suitable algorithm for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_quick_search = {\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    'LogisticRegression': LogisticRegression(),\n",
    "    'DecisionTreeRegressor': DecisionTreeRegressor(random_state=0),\n",
    "    'RandomForestRegressor': RandomForestRegressor(random_state=0),\n",
    "    'ExtraTreesRegressor': ExtraTreesRegressor(random_state=0),\n",
    "    'AdaBoostRegressor': AdaBoostRegressor(random_state=0),\n",
    "    'GradientBoostingRegressor': GradientBoostingRegressor(random_state=0),\n",
    "    'XGBRegressor': XGBRegressor(random_state=0),\n",
    "}\n",
    "\n",
    "params_quick_search = {\n",
    "    'LinearRegression': {},\n",
    "    'LogisticRegression': {},\n",
    "    'DecisionTreeRegressor': {},\n",
    "    'RandomForestRegressor': {},\n",
    "    'ExtraTreesRegressor': {},\n",
    "    'AdaBoostRegressor': {},\n",
    "    'GradientBoostingRegressor': {},\n",
    "    'XGBRegressor': {},\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the pipelines, using the above models with **default hyperparameters**\n",
    "* Parsed the train set\n",
    "* Set the performance metric as an R2 score (Regression: described in our ML business case)\n",
    "* Cross validation as 5 (rule of thumb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = HyperparameterOptimizationSearch(models=models_quick_search, params=params_quick_search)\n",
    "search.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_summary, grid_search_pipelines = search.score_summary(sort_by='mean_score')\n",
    "grid_search_summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "* The average R2 score is around **0.46**, which is lower than the **0.7** we decided in the business case.\n",
    "* The best result is **GradientBoostingRegressor**.\n",
    "* Perform an extensive search to hopefully improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stop"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model\n",
    "\n",
    "Multiple regression and classification models under consideration \n",
    "\n",
    "* sklearn.linear_model.**LinearRegression**(*, fit_intercept=True, copy_X=True, n_jobs=None, positive=False)\n",
    "* sklearn.linear_model.**LogisticRegression**(*, fit_intercept=True, copy_X=True, n_jobs=None, positive=False)\n",
    "    * *.predict_proba(X)*\n",
    "* sklearn.linear_model.**SGDRegressor**(*, fit_intercept=True, copy_X=True, n_jobs=None, positive=False)\n",
    "    * *.SGDClassifier()*\n",
    "\n",
    "List full of available and under consideration can be seen at scikitlearn [linear models](https://scikit-learn.org/stable/modules/linear_model.html#)\n",
    "\n",
    "* No one optimal model. the most appropriate seems .LogisticRegression()\n",
    "<!-- \n",
    "**.LinearRegression()** - Ordinary Least Squares\n",
    "**.SGDClassifier()** and **.SGDRegressor()** - Stochastic Gradient Descent - SGD\n",
    ".Ridge() \n",
    ".Lasso()\n",
    ".MultiTaskLasso()\n",
    ".ElasticNet()\n",
    ".MultiTaskElasticNet()\n",
    ".Lars() - Least Angle Regression\n",
    ".LassoLars()\n",
    ".OrthogonalMatchingPursuit() and orthogonal_mp()\n",
    ".BayesianRidge() - Bayesian Regression\n",
    ".ARDRegression() - Automatic Relevance Determination\n",
    "Generalized Linear Models\n",
    "**.LogisticRegression()** + **.predict_proba(X)**\n",
    ".TweedieRegressor()\n",
    ".Perceptron()\n",
    ".PassiveAggressiveClassifier() and .PassiveAggressiveRegressor()\n",
    "Robustness regression: outliers and modeling errors\n",
    ".RANSACRegressor()\n",
    ".TheilSenRegressor() and \n",
    ".HuberRegressor()\n",
    ".QuantileRegressor()\n",
    "Polynomial regression: extending linear models with basis functions\n",
    ".PolynomialFeatures() transformer -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "SGDreg = SGDRegressor()\n",
    "SGDreg.fit(X_train,y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions and Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "prediction = logrig.predict(X_test)\n",
    "print(classification_report(y_test,prediction))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
