{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# CI Portfolio Project 5 - Filter Maintenance Predictor 2022\n",
        "## **Data Cleaning Notebook**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "* Clean data in preparation for analysis\n",
        "* Create Total dataset\n",
        "* Extend Data_No references\n",
        "* Determine Correlations\n",
        "* Manage Missing Data\n",
        "* Review Outliers\n",
        "\n",
        "### Inputs\n",
        "\n",
        "* Total Dataset : `outputs/datasets/collection/PredictiveMaintenanceTotal.csv`\n",
        "\n",
        "### Outputs\n",
        "\n",
        "* Generate a cleaned **df_total** dataset saved under `outputs/datasets/cleaned`\n",
        "\n",
        "### Conclusions: \n",
        "* No missing data\n",
        "* Data clean and organized in preparation for analysis\n",
        "    * Missing data: **None**\n",
        "    * Inconsistencies found: **None**\n",
        "    * Fixes applied to incorrect data: **None**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We are assuming you will store the notebooks in a subfolder, therefore when running the notebook in the editor, you will need to change the working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "# Load Collection Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_train = pd.read_csv(f'outputs/datasets/collection/PredictiveMaintenanceTrain.csv')\n",
        "df_test = pd.read_csv(f'outputs/datasets/collection/PredictiveMaintenanceTest.csv')\n",
        "# df_total = pd.read_csv(f'outputs/datasets/collection/PredictiveMaintenanceTotal.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_test.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Exploration and Cleaning"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extend `Data_No` variable\n",
        "A comparison between sets reveals that the **Data_No** variable:\n",
        "* Is a categorical variable presented as an integer\n",
        "* Restarts at the beginning of each dataset\n",
        "\n",
        "This has the potential to confound subsequent analysis between the sets, where the analysis erroneously considers *Data_No* a discrete value &/or a duplicate entry. To help avoid confusion we alter the values in the **df_test dataset** to be a continuation from the bins seen in the **df_train dataset**.\n",
        "\n",
        "This is as simple as adding the total number of unique test bins in the df_test set to each one seen in the df_train set:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculate the total number of test sets in **df_train**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "duplicates = df_train.pivot_table(index=['Data_No'], aggfunc='size')\n",
        "df_train_total_sets = duplicates.count()\n",
        "df_train_total_sets"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Continue the numbering in the next set : **df_test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_data_no = df_test['Data_No'] + df_train_total_sets\n",
        "new_data_no"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Replace new data references into **df_test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_test['Data_No'] = new_data_no\n",
        "df_test"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create **df_total** for ease of engineering\n",
        "\n",
        "Before we continue, managing two datasets has the potential to double the workload when adding / engineering data. In the spirit of doing things once, there is value to combining the supplied datasets into one **df_total** set to work with, and split once we are happy with the cleaning and engineering stages."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Combine the **df_test** & **df_train** into **df_total**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_total = pd.concat([df_train, df_test], ignore_index=True)\n",
        "df_total"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Confirm we can recall the **df_test** & **df_train** datasets as needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_no_total = df_total['Data_No'].map(int).round(decimals=0)\n",
        "df_total['Data_No'] = data_no_total\n",
        "n = df_total['Data_No'][0:len(df_total)]\n",
        "df_train = df_total[n < 51].reset_index(drop=True)\n",
        "df_test = df_total[n > 50].reset_index(drop=True)\n",
        "del df_train['RUL']\n",
        "df_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_test"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Correlation and Power Predictive Score Analysis"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Following code derived from Code Institute [Exploratory Data Analysis Tools](https://learn.codeinstitute.net/courses/course-v1:CodeInstitute+DDA101+2021_T4/courseware/468437859a944f7d81a34234957d825b/c8ea2343476c48739676b7f03ba9b08e/) 2022."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import ppscore as pps\n",
        "\n",
        "def heatmap_corr(df, threshold, figsize=(10, 8), font_annot=8):\n",
        "    \"\"\"\n",
        "    Heatmap for pearson (linear) and spearman (monotonic) correlations to \n",
        "    visualize only those correlation levels greater than a given threshold.\n",
        "    \"\"\"\n",
        "    if len(df.columns) > 1:\n",
        "        mask = np.zeros_like(df, dtype=bool)\n",
        "        mask[np.triu_indices_from(mask)] = True\n",
        "        mask[abs(df) < threshold] = True\n",
        "\n",
        "        fig, axes = plt.subplots(figsize=figsize)\n",
        "        sns.heatmap(df, annot=True, xticklabels=True, yticklabels=True,\n",
        "                    mask=mask, cmap='viridis', annot_kws={'size': font_annot}, ax=axes,\n",
        "                    linewidth=0.01, linecolor='WhiteSmoke'\n",
        "                    )\n",
        "        axes.set_yticklabels(df.columns, rotation=0)\n",
        "        plt.ylim(len(df.columns), 0)\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def heatmap_pps(df, threshold, figsize=(10, 8), font_annot=8):\n",
        "    \"\"\"\n",
        "    Heatmap for power predictive score\n",
        "    PPS == 0 means that there is no predictive power\n",
        "    PPS < 0.2 often means that there is some relevant predictive power but it is weak\n",
        "    PPS > 0.2 often means that there is strong predictive power\n",
        "    PPS > 0.8 often means that there is a deterministic relationship in the data,\n",
        "    \"\"\"\n",
        "    if len(df.columns) > 1:\n",
        "        mask = np.zeros_like(df, dtype=bool)\n",
        "        mask[abs(df) < threshold] = True\n",
        "        fig, ax = plt.subplots(figsize=figsize)\n",
        "        ax = sns.heatmap(df, annot=True, xticklabels=True, yticklabels=True,\n",
        "                         mask=mask, cmap='rocket_r', annot_kws={'size': font_annot},\n",
        "                         linewidth=0.01, linecolor='WhiteSmoke')\n",
        "        plt.ylim(len(df.columns), 0)\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def calculate_corr_and_pps(df):\n",
        "    \"\"\"\n",
        "    Calculate the correlations and ppscore of a given dataframe\n",
        "    \"\"\"\n",
        "    df_corr_spearman = df.corr(method='spearman')\n",
        "    df_corr_pearson = df.corr(method='pearson')\n",
        "\n",
        "    pps_matrix_raw = pps.matrix(df)\n",
        "    pps_matrix = pps_matrix_raw.filter(['x', 'y', 'ppscore']).pivot(columns='x', index='y', values='ppscore')\n",
        "\n",
        "    pps_score_stats = pps_matrix_raw.query('ppscore < 1').filter(['ppscore']).describe().T\n",
        "    print('PPS threshold - check PPS score IQR to decide threshold for heatmap \\n')\n",
        "    print(pps_score_stats.round(4))\n",
        "\n",
        "    return df_corr_pearson, df_corr_spearman, pps_matrix\n",
        "\n",
        "\n",
        "def display_corr_and_pps(df_corr_pearson, df_corr_spearman, pps_matrix, CorrThreshold, PPS_Threshold,\n",
        "                      figsize=(10, 8), font_annot=8):\n",
        "    \"\"\"\n",
        "    Render the correlations and ppscore heatmaps for a given dataframe\n",
        "    \"\"\"\n",
        "    # print('\\n')\n",
        "    print('To analyze: \\n** Colinearity: how the target variable is correlated with the other features (variables)')\n",
        "    print('** Multi-colinearity: how each feature correlates among themselves (multi-colinearity)')\n",
        "\n",
        "    print('\\n')\n",
        "    print('*** Heatmap: Pearson Correlation ***')\n",
        "    print(f'It evaluates the linear relationship between two continuous variables \\n'\n",
        "          f'* A +ve correlation indicates that as one variable increases the other variable tends to increase.\\n'\n",
        "          f'A correlation near zero indicates that as one variable increases, there is no tendency in the other variable to either increase or decrease.\\n'\n",
        "          f'A -ve correlation indicates that as one variable increases the other variable tends to decrease.')\n",
        "    heatmap_corr(df=df_corr_pearson, threshold=CorrThreshold, figsize=figsize, font_annot=font_annot)\n",
        "\n",
        "    print('\\n')\n",
        "    print(f'*** Heatmap: Spearman Correlation ***')\n",
        "    print(f'It evaluates monotonic relationship \\n'\n",
        "          f'Spearman correlation coefficients range from -1 to +1.\\n'\n",
        "          f'The sign of the coefficient indicates whether it is a positive or negative monotonic relationship.\\n'\n",
        "          f'* A positive correlation means that as one variable increases, the other variable also tends to increase.')\n",
        "    heatmap_corr(df=df_corr_spearman, threshold=CorrThreshold, figsize=figsize, font_annot=font_annot)\n",
        "\n",
        "    print('\\n')\n",
        "    print('*** Heatmap: Power Predictive Score (PPS) ***')\n",
        "    print(f'PPS detects linear or non-linear relationships between two columns.\\n'\n",
        "          f'The variable on the x-axis is used to predict the corresponding variable on the y-axis.\\n'\n",
        "          f'The score ranges from 0 (no predictive power) to 1 (perfect predictive power)\\n\\n'\n",
        "          f'* PPS == 0 means that there is no predictive power\\n'\n",
        "          f'* PPS < 0.2 often means that there is some relevant predictive power but it is weak\\n'\n",
        "          f'* PPS > 0.2 often means that there is strong predictive power\\n'\n",
        "          f'* PPS > 0.8 often means that there is a deterministic relationship in the data\\n')\n",
        "    heatmap_pps(df=pps_matrix, threshold=PPS_Threshold, figsize=figsize, font_annot=font_annot)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Drop Categorical Variables, Calculate Correlations and Power Predictive Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# df_drop = df_total.drop(['Data_No', 'mass_g', 'cumulative_mass_g', 'Tt', 'filter_balance'], axis=1)\n",
        "df_drop = df_total.drop(['Data_No'], axis=1)\n",
        "# df_corr_pearson, df_corr_spearman, pps_matrix = calculate_corr_and_pps(df_total)\n",
        "df_corr_pearson, df_corr_spearman, pps_matrix = calculate_corr_and_pps(df_drop)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Pairplot** to visualize the relationships among the provided variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.pairplot(data=df_drop)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With a quick look, we notice relationships between `differential_pressure`, `RUL` and `Time`.\n",
        "\n",
        "#### Heatmaps for **df_total** dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display_corr_and_pps(df_corr_pearson = df_corr_pearson, df_corr_spearman = df_corr_spearman,\n",
        "                    pps_matrix = pps_matrix, CorrThreshold = 0, PPS_Threshold =0,\n",
        "                    figsize=(12,10), font_annot=10\n",
        "                    )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Observations\n",
        "#### Heatmap: **Pearson Correlation**\n",
        "* A linear relationship is one when a change in one variable is associated with a proportional change in the other variable \n",
        "* Positive relationships can be observed between \n",
        "    * **Differential Pressure** and **Time** plus **Flow Rate** with a negative \n",
        "    * **Flow Rate** and **RUL** plus **Time** \n",
        "* Strongly negative between **Differential Pressure** and **RUL** \n",
        "\n",
        "#### Heatmap: **Spearman Correlation**\n",
        "* A monotonic relationship is one where one variable is associated with a **change in the specific direction** of another variable. \n",
        "    * e.g. Does a positive change in value/direction X result in a positive change in the value/direction of Y?\n",
        "    * We consider Spearman’s correlation when \n",
        "        * we have pairs of continuous variables and the relationships between them don’t follow a straight line (curvilinear), and/or \n",
        "        * we have pairs of ordinal data (like time)\n",
        "\n",
        "* **Spearman's rho Values and Direction**\n",
        "    * **Differential Pressure** is strongly positively correlated to **Time**, less so **Flow Rate** and negatively correlated to **RUL**\n",
        "    * **Dust Feed** is negatively correlated to **RUL** whereas **Dust Type** is positively correlated to **RUL**\n",
        "    * **Flow Rate** is positively correlated to **Time** and **Differential Pressure** as noted above.\n",
        "\n",
        "#### Heatmap: **Power Predictive Score (PPS)**\n",
        "* Detects linear or non-linear relationships between two columns.\n",
        "* We see strong predictive power between **Dust_feed** and **RUL**, less so however still significant with **Dust_feed** and **Flow_rate**\n",
        "    * RUL as a calculation of **time** remaining, is logically affected by the volume of dust per second. The lower the flow or feed, the higher the RUL. This is however dictated by the simple fact that the filter needs to filter dust. Reducing either of the rates naturally negates the purpose of the filtering process, so we will treat it as a **confounding** relationship and as such, cannot be described in terms of correlations or associations.\n",
        "* When considering the absolute levels of the scores in the dataset, we see a weak yet strong predictive relationship between **Differential Pressure** and **RUL**\n",
        "    * Differential pressure has predictive power of RUL, whereas RUL has no predictive power on differential pressure\n",
        "    * Naturally we also see a week two way relationship between  **Differential Pressure** and **Time**\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Missing Data\n",
        "To review variables than RUL with missing data and discover their distribution and shape.\n",
        "\n",
        "Define a function to review missing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def EvaluateMissingData(df):\n",
        "    missing_data_absolute = df.isnull().sum()\n",
        "    missing_data_percentage = round(missing_data_absolute/len(df)*100, 2)\n",
        "    df_missing_data = (pd.DataFrame(\n",
        "                            data={\"RowsWithMissingData\": missing_data_absolute,\n",
        "                                   \"PercentageOfDataset\": missing_data_percentage,\n",
        "                                   \"DataType\": df.dtypes}\n",
        "                                    )\n",
        "                          .sort_values(by=['PercentageOfDataset'], ascending=False)\n",
        "                          .query(\"PercentageOfDataset > 0\")\n",
        "                          )\n",
        "\n",
        "    return df_missing_data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Review the **df_total** dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vars_with_missing_data = EvaluateMissingData(df_total)\n",
        "print(f\"* There are {vars_with_missing_data.shape[0]} variables with missing data\")\n",
        "vars_with_missing_data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As the **df_train** dataset is supplied without values for `RUL`, we can extract the **df_test** set checked separately to confirm the function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_no_total = df_total['Data_No'].map(int).round(decimals=0)\n",
        "df_total['Data_No'] = data_no_total\n",
        "n = df_total['Data_No'][0:len(df_total)]\n",
        "df_train = df_total[n < 51].reset_index(drop=True)\n",
        "df_test = df_total[n > 50].reset_index(drop=True)\n",
        "# data_no_total = df_total['Data_No'].map(str)\n",
        "# df_total['Data_No'] = data_no_total\n",
        "del df_train['RUL']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check for missing data and return a **Pandas Profile Report** on the variable with missing "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pandas_profiling import ProfileReport\n",
        "\n",
        "def name_dataframe(data):\n",
        "    \"\"\" To identify which dataframe is being accessed \"\"\"\n",
        "    name =[n for n in globals() if globals()[n] is data][0]\n",
        "    print('Dataframe name: %s' % name)\n",
        "\n",
        "for df in (df_train, df_test):\n",
        "    vars_with_missing_data = df.columns[df.isna().sum() > 0].to_list()\n",
        "    if vars_with_missing_data:\n",
        "        profile = ProfileReport(df=df[vars_with_missing_data], minimal=True)\n",
        "        profile.to_notebook_iframe()\n",
        "    else:\n",
        "        name_dataframe(df)\n",
        "        print('There are no variables with missing data')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Outliers in differential pressure observations"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In each bin we notice fluctuations in the **differential pressures** measure. These may be considered outliers as the pressure gradient across the filter needs time to stabilize. We have considered three main methods to deal with these observations:\n",
        "* Log transformation\n",
        "* Winsorize method\n",
        "* Dropping the outliers\n",
        "\n",
        "These will be handled in the [feature engineering](https://github.com/roeszler/filter-maintenance-predictor/blob/main/jupyter_notebooks/03_FeatureEngineering.ipynb) notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_total[df_total['Data_No'] == 96]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "# Save Datasets"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save the files to /cleaned folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKlnIozA4eQO",
        "outputId": "fd09bc1f-adb1-4511-f6ce-492a6af570c0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "try:\n",
        "  os.makedirs(name='outputs/datasets/cleaned')\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "\n",
        "df_total.to_csv(f'outputs/datasets/cleaned/dfCleanTotal.csv',index=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusions and Next steps\n",
        "\n",
        "#### Observations:\n",
        "* Possible relationships to consider:\n",
        "    * **Differential Pressure**, **Time**, **RUL**\n",
        "    * **Dust Feed, Flow and Type** to **RUL**\n",
        "\n",
        "#### Conclusions: \n",
        "* No missing data\n",
        "* Data clean and organized in preparation for analysis\n",
        "    * Missing data: **None**\n",
        "    * Inconsistencies found: **None**\n",
        "    * Fixes applied to incorrect data: **None**\n",
        "* Remaining Useful Life can be accurately calculated from Remaining Filter Balance, Time and/or Total Time parameters. \n",
        "\n",
        "#### Next Steps:\n",
        "* Correlation Study\n",
        "* Feature Engineering\n",
        "    * Additional calculations to add interpretative value to the datasets\n",
        "    * Confirm RUL calculation methodology\n",
        "    * "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
